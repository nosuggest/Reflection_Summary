# 解释方差：
期望值与真实值之间的波动程度，衡量的是**稳定性**

# 解释偏差：
期望值与真实值之间的一致差距，衡量的是**准确性**

# 为什么要有偏差和方差？
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和
**Err = bias + var + irreducible error** 

以回归任务为例,其实更准确的公式为：**Err = bias^2 + var + irreducible error^2** 

符号的定义：一个真实的任务可以理解为Y=f(x)+e，其中f(x)为规律部分，e为噪声部分
- 训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。


# 什么情况下产生方差？
- 过高复杂度的模型，对训练集进行过拟合
    - 带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差
    - 更加形象的理解就是用一条高次方程去拟合线性数据

# 如何解决高方差问题？
- 在模型复杂程度不变的情况下，增加更多数据
- 在数据量不变的情况下，减少特征维度
- 在数据和模型都不变的情况下，加入正则化

# 以上方法是否一定有效？
- 增加数据如果和原数据分布一致，无论增加多少必定解决不了高方差
    - smote对样本进行扩充是否必定可以避免高方差？
    - 过采样是否解决高方差问题？
- 减少的特征维度如果是共线性的维度，对原模型没有任何影响
    - 罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？
- 正则化通常都是有效的

# 如何解决高偏差问题？
- 尝试获得更多的特征
    - 从数据入手，进行特征交叉，或者特征的embedding化
- 尝试增加多项式特征
    - 从模型入手，增加更多线性及非线性变化，提高模型的复杂度
- 尝试减少正则化程度λ

# 以上方法是否一定有效？
- 特征越稀疏，高方差的风险越高
- 多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换
- 正则化通常都是有效的

# 遇到过的机器学习中的偏差与方差问题？
- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。
- 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。

# 遇到过的深度学习中的偏差与方差？
神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；
因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。
- dropout
- dense中的normalization
- 数据的shuffle