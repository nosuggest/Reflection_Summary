# 为什么需要对数据进行变换？
- 避免异常点：比如对连续变量进行份桶离散化
- 可解释性或者需要连续输出：比如评分卡模型中的iv+woe
- 使得原始数据的信息量更大：比如log/sqrt变换

# 归一化和标准化之间的关系？
- 归一化(max-min)
    - 缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程

- 标准化(z-score)
    - 缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中

- 作用
    - 解决部分模型由于数据值域不同对模型产生的影响，尤其是距离模型
    - 更快的收敛
    - 去量纲化
    - 避免数值计算溢出
    
- 总结
    - 异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降
    - 分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下
    - 上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动
    - 值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定
    - 模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换

- 常用模型    
    - knn：计算距离，不去量冈则结果受值域范围影响大
    - neural network：梯度异常问题+激活函数问题
        
# 连续特征常用方法
- 截断
    - 连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)
    - 参考异常点里面的outlier识别，以最大值填充或者以None
- 二值化
    - 数据分布过于不平衡
    - 空值/异常值过多
- 分桶
    - 小范围连续数据内不存在逻辑关系，比如31岁和32岁之间不存在明显的差异，可以归为一类
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2hcomdwj30k60djdh4.jpg)
- 离散化    
    - 数值无意义，比如学历、祖籍等等
- 缩放
    - z-score标准化
    - min-max归一化
    - 范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg)
        - L1范数
        - L2范数
    - 平方根缩放
    - 对数缩放
        - 对数缩放适用于处理长尾分且取值为正数的数值变量
            - 它将大端长尾压缩为短尾，并将小端进行延伸
        - 可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)-log(y)
        - 可以把有偏分布修正为近似正太分布
    - Box-Cox转换
        - ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfjjwir3j309m038gln.jpg)
        - 通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，Box-Cox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件
- 特征交叉
    - 人为分段交叉
        - 提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释
        - 离散变量的交并补
        - 连续变量的点积，attention类似
        - 交叉中需要并行特征筛选的步骤
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2rf5aawj30ka0j6q48.jpg)
    - 自动组合
        - FM/FFM中的矩阵点积
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2un2ckzj30eb07jaan.jpg)
        - Neural Network里面的dense
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2vb1nnij30co07lt94.jpg)
    - 条件选择
        - 通过树或者类似的特征组合模型去做最低熵的特征选择
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2t44a2mj30im0bt0tz.jpg)
            - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2v20wjbj30hs0dx0tq.jpg)
            
- 非线性编码
    - 核向量进行升维
    - 树模型的叶子结点的stack
    - 谱聚类/pca/svd等信息抽取编码
    - lda/EM等分布拟合表示

# 离散特征常用方法
- one-hot-encoder
- 分层编码
    - 有一定规律的类别数据，邮政编码，手机号等等
- 计数编码
    - 将类别特征用其对应的计数来代替,这对线性和非线性模型都有效
    - 对异常值比较敏感,特征取值有可能冲突
- 计数排名编码
    - 解决上述问题，以排名代替值
- Embedding
    - 对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题
- 类别特征之间交叉组合
    - 笛卡尔交叉
- 类别特征和数值特征之间交叉组合
    - 均值、中位数、标准差、最大值和最小值
    - 分位数、方差、vif值、分段冲量

# 文本特征
- 预处理手段有哪些？
    - 将字符转化为小写
    - 分词
    - 去除无用字符
    - 繁体转中文
    - 去除停用词
    - 去除稀有词
    - 半角全角切换
    - 错词纠正
    - 关键词标记
        - TF-IDF
        - LDA
        - LSA
    - 提取词根
    - 词干提取
    - 标点符号编码
    - 文档特征
    - 实体插入和提取
    - 文本向量化
        - word2vec
        - glove
        - bert
    - 文本相似性
- 如何做样本构造？
    - 按标点切分
    - 按句切分
    - 对话session切分
    - 按文章切分
    - 按场景切分
- 分词过程中会考虑哪些方面？
    - 词性标注
    - 词形还原和词干提取
        - 词形还原为了通用性特征的提取
        - 词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义
- 文本中的统计信息一般有哪些？
    - 直接统计值：
        - 文本的长度
        - 单词个数
        - 数字个数
        - 字母个数
        - 大小写单词个数
        - 大小写字母个数
        - 标点符号个数
        - 特殊字符个数
        - 数字占比
        - 字母占比
        - 特殊字符占比
        - 不同词性个数    
    - 直接统计值的统计信息：
        - 最小最大均值方差标准差
        - 分位数，最早/最晚出现位置
- 直接对文本特征进行整理手段有哪些？   
    - N-Gram模型
        - 将文本转换为连续序列，扩充样本特征
        - 连续语意的提取
    - TF-IDF
        - 权重评分，去除掉一些低重要性的词，比如每篇文章都出现的"的"，"了"
    - LDA
        - 主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系
    - 相似度
        - 余弦相似度
        - Jaccard相似度
            - 共现性
        - Levenshtein(编辑距离)
            - 文本近似程度
        - 海林格距离
            - 用来衡量概率分布之间的相似性
        - JSD
            - 衡量prob1 和 prob2两个分布的相似程度
    - 向量化
        - word2vec
        - glove
        - bert
- 文本处理有大量可以讲，可以谈的，以上只是做了一个最简单的汇总，详细的会在自然语言处理的专题一条一条分析，面试官也一定会就每个知识点进行展开

# 画一个最简单的最快速能实现的框架
- 建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的
- 建议从简单的开始，然后面试官说还有其他方法么？再做延展：
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mzet073zj31jw0u046i.jpg)