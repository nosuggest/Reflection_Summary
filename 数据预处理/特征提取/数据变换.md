# 为什么需要对数据进行变换？
- 避免异常点：比如对连续变量进行份桶离散化
- 可解释性或者需要连续输出：比如评分卡模型中的iv+woe
- 使得原始数据的信息量更大：比如log/sqrt变换

# 归一化和标准化之间的关系？
- 归一化
    - 通过改变数值范围，使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类球形
    - 作用：
        - 提高精度
        - 提高速度
- 标准化
    - 通过在不改变原始分布的情况下，使得不同分布之间可以相互比较，常用伸缩变换，内部的相对分布是不会变的
    - 作用：
        - 不同的度量之间不仅仅是数值而且是空间内可比
        - 不改变原始分布
- 总结
    - 通常，我会在特征之间差异范围特别大的时候，比如一些特征是\[0,1]一些特征是\[-e100,e100]进行特征之间的交叉的时候会考虑z-score，而当特征都差不多范围，而我一般也会使用类似min-max进行加速收敛        

# 连续特征常用方法
- 截断
    - 连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)
    - 参考异常点里面的outlier识别，以最大值填充或者以None
- 二值化
    - 数据分布过于不平衡
    - 空值/异常值过多
- 分桶
    - 小范围连续数据内不存在逻辑关系，比如31岁和32岁之间不存在明显的差异，可以归为一类
- 离散化    
    - 数值无意义，比如学历、祖籍等等
- 缩放
    - z-score标准化
    - min-max归一化
    - 范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg)
        - L1范数
        - L2范数:
    - 平方根缩放
    - 对数缩放
        - 对数缩放适用于处理长尾分且取值为正数的数值变量
            - 它将大端长尾压缩为短尾，并将小端进行延伸
    - Box-Cox转换
        - ![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfjjwir3j309m038gln.jpg)
        - 通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，Box-Cox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件
- 缺失值填充
    - 直接填充
        - 均值
        - 中位数
        - 众数
        - 分位数
    - 模型插值
        - 有效性存疑，取决于特征列数
    - 新增列空值特征
        - 连续特征分桶
        - 离散特征新增category
- 特征交叉
    - 人为分段交叉
        - 提升模型的拟合能力，使基向量更有表示能力
        - 离散变量的交并补
        - 连续变量的点积
    - 自动组合
        - FM/FFM中的矩阵点积
        - Neural Network里面的dense
    - 条件选择
        - 通过树或者类似的特征组合模型去做最低熵的特征选择
- 非线性编码
    - 核向量进行升维
    - 树模型的叶子结点的stack
    - 谱聚类/pca/svd等信息抽取编码
    - lda/EM等分布拟合表示

# 离散特征常用方法
- one-hot-encoder
- 分层编码
    - 有一定规律的类别数据，邮政编码，手机号等等
- 计数编码
    - 将类别特征用其对应的计数来代替,这对线性和非线性模型都有效
    - 对异常值比较敏感,特征取值有可能冲突
- 计数排名编码
    - 解决上述问题，以排名代替值
- Embedding
    - 对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题
- 类别特征之间交叉组合
    - 笛卡尔交叉
- 类别特征和数值特征之间交叉组合
    - 均值、中位数、标准差、最大值和最小值
    - 分位数、方差、vif值、分段冲量

# 文本特征
- 预处理手段有哪些？
    - 将字符转化为小写
    - 分词
    - 去除无用字符
    - 繁体转中文
    - 去除停用词
    - 去除稀有词
    - 半角全角切换
    - 错词纠正
    - 关键词标记
        - TF-IDF
        - LDA
        - LSA
    - 提取词根
    - 词干提取
    - 标点符号编码
    - 文档特征
    - 实体插入和提取
    - 文本向量化
        - word2vec
        - glove
        - bert
    - 文本相似性
- 如何做样本构造？
    - 按标点切分
    - 按句切分
    - 对话session切分
    - 按文章切分
    - 按场景切分
- 分词过程中会考虑哪些方面？
    - 词性标注
    - 词形还原和词干提取
        - 词形还原为了通用性特征的提取
        - 词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义
- 文本中的统计信息一般有哪些？
    - 直接统计值：
        - 文本的长度
        - 单词个数
        - 数字个数
        - 字母个数
        - 大小写单词个数
        - 大小写字母个数
        - 标点符号个数
        - 特殊字符个数
        - 数字占比
        - 字母占比
        - 特殊字符占比
        - 不同词性个数    
    - 直接统计值的统计信息：
        - 最小最大均值方差标准差
        - 分位数，最早/最晚出现位置
- 直接对文本特征进行整理手段有哪些？   
    - N-Gram模型
        - 将文本转换为连续序列，扩充样本特征
        - 连续语意的提取
    - TF-IDF
        - 权重评分，去除掉一些低重要性的词，比如每篇文章都出现的"的"，"了"
    - LDA
        - 主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系
    - 相似度
        - 余弦相似度
        - Jaccard相似度
            - 共现性
        - Levenshtein(编辑距离)
            - 文本近似程度
        - 海林格距离
            - 用来衡量概率分布之间的相似性
        - JSD
            - 衡量prob1 和 prob2两个分布的相似程度
    - 向量化
        - word2vec
        - glove
        - bert
- 文本处理有大量可以讲，可以谈的，以上只是做了一个最简单的汇总，详细的会在自然语言处理的专题一条一条分析，面试官也一定会就每个知识点进行展开

# 画一个最简单的最快速能实现的框架
- 建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的
- 建议从简单的开始，然后面试官说还有其他方法么？再做延展：
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mzet073zj31jw0u046i.jpg)