# 介绍残差网络
- 常见结构，CV里面用的比较多
    - y=F(x)+x
    - y=F(x)+indentity `*` x

# 残差网络为什么能解决梯度消失的问题
- ![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is4x344xj304j01jglf.jpg)
- ![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8cigikj305s0180sl.jpg)
    - 虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题

# 残差网络残差作用
- 防止梯度消失
- 恒等映射使得网络突破层数限制，避免网络退化
- 对输出的变化更敏感
    - X=5;F(X)=5.1;F(X)=H(X)+X=>H(X)=0.1
    - X=5;F(X)=5.2;F(X)=H(X)+X=>H(X)=0.2
    - H(X)变换了100%，去掉相同的主体部分，从而突出微小的变化

# 你平时有用过么？或者你在哪些地方遇到了
- 我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络
- Bert和Transform中attention部分残差网络用的比较频繁

