# LR推导，基础5连问
- 基础公式
    - f(x) = wx + b
    - y = sigmoid(f(x))
    - 可以看作是一次线性拟合+一次sigmoid的非线性变化
- 伯努利过程
    - 对于lr来说事情只有发生和不发生两种可能，对于已知样本来说，满足伯努利的概率假设：
        - p(y=1/x,θ) = h(θ,x)
        - p(y=0/x,θ) = 1-h(θ,x)
        - p(y/x,θ) = h(θ,x)^y · (1-h(θ,x))^(1-y) 
            - 第i个样本正确预测的概率如上可得
    - 几率odds
        - 数据特征下属于正例及反例的比值
        - ln(y/(1-y))
- 极大似然
    - 第i个样本正确预测的概率如上可得每条样本的情况下
    - 综合全部样本发生的概率都要最大的话，采取极大似然连乘可得：
        - ∏(h(θ,x)^y · (1-h(θ,x))^(1-y))
- 损失函数
    - 通常会对极大似然取对数，得到损失函数，方便计算
        - ∑ylogh(θ,x)+(1-y)log(1-h(θ,x))最大
        - 及-1/m · ∑ylogh(θ,x)+(1-y)log(1-h(θ,x))最小    
- 梯度下降
    - 损失函数求偏导，更新θ
    - θj+1 = θj - ∆·∂Loss/∂θ =θj - ∆·1/m·∑x·(h-y)
        - ∆为学习率

# LR明明是分类模型为什么叫回归？
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归

# 为什么LR可以用来做CTR预估？
1. 点击行为为正向，未点击行为为负向，ctr需要得到点击行为的概率，lr可以产出正向行为的概率，完美match
2. 实现简单，方便并行，计算迭代速度很快
3. 可解释性强，可结合正则化等优化方法

# 满足什么样条件的数据用LR最好？
- 特征之间尽可能独立
    - 不独立所以我们把不独立的特征交叉了
        - 还记得FM的思路？
- 离散特征
    - 连续特征通常没有特别含义，31岁和32岁差在哪？
    - 离散特征方便交叉考虑
    - 在异常值处理上也更加方便
    - 使的lr满足分布假设
        - 什么分布假设？
- 在某种确定分类上的特征分布满足高斯分布
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wh7dd6bkj310w034gmb.jpg)
    - C1和C2为正负类，观测样本中该特征在正负类中出现概率的比值满足线性条件的前提就是P服从正太分布
        - 实际中不满足的很多，不满足我们通常就离散化，oneHotEncode

此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中

# LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？
- 思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开
- 思路二：Exponential model 的形式是这样的：假设第i个特征对第k类的贡献是![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmq11x6zj300s00f3y9.jpg)，则数据点![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmqpotqfj302700imwx.jpg)属于第k类的概率正比于![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmrp4qv4j305b00i0sj.jpg)。
    - 二分类上：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmsc1vfkj30jm036wet.jpg)
    - 化简即为sigmoid

# Sigmoid函数到底起了什么作用？
- 数据规约：\[0,1]
- 解决单纯的线性不可分问题
- 线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感    

# LR为什么要使用极大似然函数作为损失函数？那为什么不选平方损失函数的呢
- 极大似然保证训练样本出现的概率最大，数据量越大越符合真实分布，拟合模型越鲁棒
- 更新速度只与真实的x和y相关，与激活函数无关，更新平稳
    - 比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新

# LR中若标签为+1和-1，损失函数如何推导？
- way1:把0-1的sigmoid的lr结果Y映射为2y-1，推导不变
- way2:把激活函数换成tanh，因为tanh的值域范围为\[-1,1],满足结果，推导不变    
- way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可    

# 如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
# 为什么要避免共线性？
- 如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果
- 每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了
- 增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠
- 泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差

# LR可以用核么？可以怎么用？
结论：可以，加l2正则项后可用

原因：
- 核逻辑回归，需要把拟合参数w表示成z的线性组合及representer theorem理论。这边比较复杂，待更新，需要了解：
    - w拆解的z的线性组合中的系数α来源
    - representer theorem 的证明
        - 凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明
    - 如何将将W*表示成β的形式带到我们最佳化的问题

# LR中的L1/L2正则项是啥？
- L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wp6wwlbzj303400ygle.jpg) ,u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpg57x1gj301g0133ya.jpg)项
- L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项
    
# 我的总结
- 逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的