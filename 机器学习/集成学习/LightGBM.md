# XGboost缺点
- 每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间
- 预排序方法需要保存特征值，及特征排序后的索引结果，占用空间
- level-wise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数

# LightGBM对Xgboost的优化
- 将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点
    - 优点：时间开销由O(features)降低到O(bins)
    - 缺点：很多数据精度被丢失，相当于用了正则
- 利用leaf-wise代替level-wise
    - 每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环
- 直方图做差加速

# LightGBM亮点
- 单边梯度采样 Gradient-based One-Side Sampling (GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益
- 互斥稀疏特征绑定Exclusive Feature Bundling (EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]
