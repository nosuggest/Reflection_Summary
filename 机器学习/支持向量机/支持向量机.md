# 简单介绍SVM?
- 从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2·||W||·||W||)  subject to：y(wx+b)>=1，其中||·||为2范数
- 然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题
- 最后再利用SMO（序列最小优化）来解决这个对偶问题

# 什么叫最优超平面？
- 两类样本分别分割在该超平面的两侧
- 超平面两侧的点离超平面尽可能的远

# 什么是支持向量？
在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点

# SVM 和全部数据有关还是和局部数据有关?
局部

# 加大训练数据量一定能提高SVM准确率吗？
支持向量的添加才会提高，否则无效

# 如何解决多分类问题？
对训练器进行组合。其中比较典型的有一对一，和一对多

# 可以做回归吗，怎么做？
可以，把loss函数变为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93jrdscfrj309l011dfo.jpg)

# SVM 能解决哪些问题？
- 线性问题
    - 对于n为数据，找到n-1维的超平面将数据分成2份。通过增加一个约束条件： 要求这个超平面到每边最近数据点的距离是最大的
- 非线性问题
    -  SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器

# 介绍一下你知道的不同的SVM分类器？
- 硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器
- 软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器
- kernel SVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器

# 什么叫软间隔？
软间隔允许部分样本点不满足约束条件： 1<y(wx+b)

# SVM 软间隔与硬间隔表达式
- 硬间隔： ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93ha4y4glj3094011a9w.jpg)
- 软间隔： ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93hae5o8lj30d701f0sn.jpg)


# SVM原问题和对偶问题的关系/解释原问题和对偶问题？
- svm原问题是：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg)
- svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)
    - 拉格朗日乘子法：求f的最小值时，有h=0的限制条件，那么就构造∑λh+f=Loss,作为新loss 
    - 引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件
    - 在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数

# 为什么要把原问题转换为对偶问题？
- 因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效
- 引入了核函数

# 为什么求解对偶问题更加高效？
- 原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题
- 因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0

# alpha系数有多少个？
样本点的个数

# KKT限制条件，KKT条件有哪些，完整描述
- ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的 KKT(Karush-Kuhn-Tucker) 条件
- KKT乘子λ>=0

# 引入拉格朗日的优化方法后的损失函数解释
- 原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg)
- 优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg)
    - 要求KKT乘子λ>=0

# 核函数的作用是啥
核函数能够将特征从低维空间映射到高维空间， 这个映射可以把低维空间中不可分的两类点变成高维线性可分的

# 核函数的种类和应用场景
- 线性核函数：主要用于线性可分的情形。参数少，速度快。
- 多项式核函数：
- 高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。
- sigmoid 核函数：
- 拉普拉斯核函数：

# 如何选择核函数
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候

# 常用核函数的定义？
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：
1) 线性：K(v1,v2) = <v1,v2>
2) 多项式：K(v1,v2) = (r<v1,v2>+c)^n
3) Radial basis function：K(v1,v2) = exp(-r||v1-v2||^2)
4) Sigmoid：tanh(r<v1,v2>+c)

# 核函数需要满足什么条件？
Mercer定理：核函数矩阵是对称半正定的

# 为什么在数据量大的情况下常常用lr代替核SVM？
- 计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2)
- 在使用核函数的时候参数假设全靠试，时间成本过高

# 高斯核可以升到多少维？为什么
无穷维
e的n次方的泰勒展开得到了一个无穷维度的映射

# SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整

# 各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归
- 线性问题：
    - 线性：
        - 逻辑回归，线性svm
    - 非线性：
        - 贝叶斯，决策树，核svm，DNN
- 数据问题：
    - 数据量大特征多：
        - 逻辑回归
        - 决策树算法
    - 数据量少特征少：
        - 核svm
- 缺失值多：
    - 树模型

# Linear SVM 和 LR 有什么异同？
- LR是参数模型，SVM为非参数模型。
- LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。
- 在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。
- LR的模型相对简单，在进行大规模线性分类时比较方便。